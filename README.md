#  Voice-to-Voice AI Assistant

Voice Assistant for Raspberry Pi, integrating:

- ğŸ™ï¸ **Whisper.cpp** â€” Speech-to-Text (STT)
- ğŸ§  **TinyLLaMA (via llama.cpp)** â€” Language Model for text generation
- ğŸ”Š **Piper TTS** â€” Text-to-Speech (TTS)

Built as a **Bachelor's Major Project**, it delivers real-time, low-latency voice interactions on constrained hardware.

---

## ğŸ› ï¸ Hardware Requirements

- Raspberry Pi 4 (4GB or 8GB)
- USB Microphone (or onboard mic)
- Speakers (3.5mm jack or HDMI)
- MicroSD card (32GB+ recommended)


---

## ğŸ“ Project Structure

```
pill/
â”œâ”€â”€ audio/ # Temporary audio files
â”‚ â””â”€â”€ speech.wav
â”œâ”€â”€ bin/ # Executable scripts
â”‚ â”œâ”€â”€ run.sh # Main voice-to-voice pipeline
â”‚ â”œâ”€â”€ speak # TTS wrapper
â”‚ â””â”€â”€ tokens # LLaMA wrapper
â”œâ”€â”€ stt/
â”‚ â”œâ”€â”€ bin/ # Whisper binary
â”‚ â””â”€â”€ models/ # Whisper model (e.g., ggml-tiny.bin)
â”œâ”€â”€ llm/
â”‚ â”œâ”€â”€ bin/ # llama.cpp binary and shared libs
â”‚ â””â”€â”€ models/ # GGUF LLaMA models
â”œâ”€â”€ tts/
â”‚ â”œâ”€â”€ piper/ # Piper binary
â”‚ â””â”€â”€ voice/ # ONNX voice models
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```
---


This pipeline will:
1. Record audio from mic
2. Transcribe with Whisper
3. Generate reply with LLaMA
4. Speak with Piper



---

## ğŸ” Workflow Overview

### ğŸ—£ï¸ 1. User Speaks
- Audio is recorded with a microphone and saved to `audio/speech.wav`.

### ğŸ§ 2. Speech-to-Text (STT)
- Audio is transcribed using Whisper.cpp with a small model like `ggml-tiny.bin`.

### ğŸ§  3. Text Generation with LLaMA
- Transcribed text is passed to TinyLLaMA via `llama.cpp`.
- A quantized model (Q4/Q5) ensures fast inference on Raspberry Pi.

### ğŸ”Š 4. Text-to-Speech (TTS)
- Piper TTS synthesizes the response using a pre-downloaded ONNX voice model.

---

## ğŸ› ï¸ Step-by-Step Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/rsvn/pill.git
cd pill
```

### 2ï¸âƒ£ Install Python Dependencies

```bash
pip install -r requirements.txt
```

### ğŸ **Set up a virtual environment**

  Using `venv`:

```shell
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```



#### run set.sh for installing the required models or bulid it your self from below repo
```
./setup.sh

```

### 3ï¸âƒ£ Build/Download Binaries

- **Whisper.cpp:** Build `main` as `whisper-cli`
- **llama.cpp:** Build `main` as `llama-cli`
- **Piper:** Build binary and required shared libs

Place them in:
```
stt/bin/whisper-cli
llm/bin/llama-cli
tts/piper/piper
```

### 4ï¸âƒ£ Download Models

#### Whisper Model

```bash
wget https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-tiny.bin -P stt/models/
```

#### TinyLLaMA (GGUF)

- Source: https://huggingface.co/cognitivecomputations/TinyLlama-1.1B-Chat-v1.0

Ensure format is `.gguf` and quantized (e.g., Q4_K_M):

```bash
# Example (after conversion if needed)
mv <downloaded>.gguf llm/models/tinyllama_1b_q4_chat.gguf
```

#### Piper Voice Model

```bash
wget https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US-libritts_r-medium.onnx -P tts/voice/libritts_r/
```

---

## âš™ï¸ Environment Configuration

Ensure `llm/bin` is in your shared library path:

```bash
export LD_LIBRARY_PATH=llm/bin:$LD_LIBRARY_PATH
```



---

## if any thing breaks,make sure three api end points are working properly

## ğŸ§ª Manual Testing

### Whisper (STT)

```bash
./stt/bin/whisper-cli ../audio/speech.wav --model ../stt/models/ggml-tiny.bin
```

### LLaMA (LLM)

```bash
./llm/bin/llama-cli -m ../llm/models/tinyllama_1b_q4_chat.gguf -p "Hello, who are you?" -n 50
```

### Piper (TTS)

 ğŸ¤ **Install Local TTS - Piper**

   _A faster and lightweight alternative to MeloTTS_

   ***Download the Piper Binary and the voice from Github***

   Use the following [link](https://github.com/rhasspy/piper) to install Piper Binary for your operating system.

   Use the following [link](https://github.com/rhasspy/piper?tab=readme-ov-file#voices) to download Piper voices.
   Each voice will have two files:
   | `.onnx` | Actual voice model |
   | `.onnx.json` | Model configuration |

   For example:

   ```shell
   models/en_US-lessac-medium/
   â”œâ”€â”€ en_US-lessac-medium.onnx
   â”œâ”€â”€ en_US-lessac-medium.onnx.json
   ```

### test from the cli

```bash
./tts/piper/piper --model ../tts/voice/libritts_r/en_US-libritts_r-medium.onnx --text "Hello, I am your Pi assistant."

---

```
## ğŸ”§ Detailed Description of `bin/` Scripts

The `bin/` folder in the PILL project contains the key scripts that power the full voice-to-voice AI pipeline. Each script is responsible for one or more stages of the interaction loop: capturing audio, generating a response, and speaking it back to the user.## ğŸ“ Main Scripts & Modules (bin/ and core components)

This section documents the purpose of each key script and module in the PILL project.

### ğŸ”§ `bin/` - Executable Scripts

| Script/File     | Description |
|-----------------|-------------|
| `run-server.sh`        | Main orchestrator script that handles the complete voice-to-voice assistant pipeline: audio recording â†’ transcription â†’ LLM response â†’ TTS output. |
| `speak`         | Wrapper for Piper TTS. Converts text input to spoken audio using the selected voice model. |
| `tokens`        | Wrapper for LLaMA model. Takes user prompts and runs the language model to generate text responses. |

---

### âš™ï¸ Core Components

| Module/Folder     | Description |
|-------------------|-------------|
| `core/`           | Contains the logic for external data integrations such as news scraping and data preprocessing. Ideal for enriching responses with real-world context such as news,weather,maps,wiki. |
| `auto-comp/`      | Provides auto-completion or predictive typing features. Helps improve user experience by suggesting or auto-filling text. |
| `get.py`          | Script to retrieve dynamic content like weather updates, news headlines, or Wikipedia summaries. Acts as a smart data fetcher for the assistant. |
| `run-server/`        | A script or module to run all major features or tests together. Useful for demos or integration testing. |
| `simple-cli/`     | A basic command-line interface to interact with the assistant via text. Does not involve voice processing. Useful for debugging or quick tests. |
| `spell/`          | Provides gpt response while taking input in the form of text from the user |
| `tokenizer/`      | Utility functions for tokenizing input or output text for compatibility with the LLaMA model. Handles text formatting, splitting, and pre-processing. |




---

### ğŸ—£ï¸ `run-server.sh` â€“ Master Orchestrator

This is the **main script** that ties the entire system together and initiates the voice assistant workflow.

#### Responsibilities:
- Records a voice input from the microphone and saves it as `audio/speech.wav`
- Transcribes the audio to text using Whisper (`whisper-cli`)
- Feeds the transcribed prompt to the LLM via the `tokens` wrapper script
- Converts the generated text response into speech using the `speak` wrapper
- Plays back the synthesized voice to the user


---
## ğŸš€ Run the Assistant

```bash
cd bin
./run-server
```

---
##  ScreenShots

![Work flow](assets/work-flow.png)
![rag architecture](assets/rag.png)
![Terminal Interface](assets/terminal.png)
![Raspberry Pi](assets/pi.png)
![Setup](assets/demo.gif)



## ğŸ“… Last Updated
sep 6, 2025

####  -- rsvn
