#!/bin/bash

# ---------------- Config ----------------
laptop_ip="192.168.231.189"
port=8080
llama_url="http://$laptop_ip:$port/completion"

whisper="../stt/bin/whisper-cli"
txt_model="../stt/models/ggml-tiny.bin"
audio_file="../audio/speech.wav"

piper="../tts/piper/piper"
tts_model="../tts/voice/libritts_r/en_US-libritts_r-medium.onnx"
beep_sound="../audio/beep/bing.mp3"

echo "ğŸ™ï¸ Pill Assistant (Client Mode - using laptop's LLaMA) Started"

# ---------------- Main Loop ----------------
while true; do
  echo ""
  mpg123 -q "$beep_sound"
  echo "ğŸ”´ Listening for your voice..."

  # 1. ğŸ”” Beep before recording
  # 2. Record audio
  arecord -q -f S16_LE -r 16000 -c 1 -d 5 "$audio_file"

  # 3. Check for silence using sox (optional, if implemented)
  # silent_check=$(sox "$audio_file" -n stat 2>&1 | grep "Maximum amplitude" | awk '{print $3}')
  # if [[ $(echo "$silent_check < 0.01" | bc) -eq 1 ]]; then
  #   echo "âš ï¸ Silence detected. Skipping..."
  #   continue
  # fi

  echo "ğŸ§  Transcribing speech to text..."
  "$whisper" -m "$txt_model" -f "$audio_file" -otxt > /dev/null 2>&1
  transcript=$(cat "${audio_file}.txt")

  if [ -z "$transcript" ]; then
    echo "âš ï¸ Didn't catch that. Please speak clearly."
    continue
  fi

  echo "ğŸ“œ You said: \"$transcript\""

  # 4. Generate full LLM prompt using query_handler.py
  prompt=$(python3 ./query_handler.py "$transcript")

  # 5. Send prompt to LLaMA server
  echo "ğŸ¤– Sending to LLaMA server on $laptop_ip..."
  response=$(curl -s "$llama_url" \
    -X POST \
    -H "Content-Type: application/json" \
    -d "$(jq -nc --arg prompt "$prompt" '{"prompt": $prompt, "n_predict": 128}')" \
    | jq -r '.content')

  echo "ğŸ—£ï¸ Response: $response"

  # 6. Speak it out
  echo "ğŸ”Š Speaking out loud..."
  echo "$response" | "$piper" --model "$tts_model" --output-raw 2>/dev/null | aplay -q -f S16_LE -r 22050

  echo "ğŸ” Ready for the next query."
done
