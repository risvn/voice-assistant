## **ğŸ§  RAG (Retrieval-Augmented Generation) with ChromaDB \+ SentenceTransformers**

This project showcases a straightforward, local implementation of **Retrieval-Augmented Generation (RAG)**, leveraging the following:

* ğŸ’¬ **sentence-transformers**: For generating sophisticated text embeddings.  
* ğŸ—ƒï¸ **ChromaDB**: Serving as a persistent, local vector database.  
* ğŸ§¾ **Text input**: Sourced directly from .txt files.  
* ğŸ” **Semantic search**: To enrich queries with highly relevant contextual information.

## ---

**ğŸ“š What is RAG?**

**RAG** is an innovative technique that involves **retrieving pertinent information from a custom knowledge base** (such as your own documents or notes) and subsequently **augmenting the prompt** provided to a language model.

This powerful approach empowers the language model to:

* Provide answers deeply **grounded in your proprietary data**.  
* Effectively **circumvent limitations of its context window** and mitigate **hallucination issues**.  
* Operate seamlessly **locally, without requiring an internet connection**.

## ---

**ğŸ› ï¸ How It Works**

### **1\. âœ‚ï¸ Chunking and Embedding**

* Text content from .txt files located in the ./data folder is systematically divided into manageable **chunks** (e.g., approximately 500 words each).  
* Each of these chunks is then transformed into a numerical vector representation using a SentenceTransformer model, such as "all-MiniLM-L6-v2".

### **2\. ğŸ§  Vector Storage in ChromaDB**

* The generated embeddings are securely stored in a local **ChromaDB** database, which resides in the ./chroma\_db directory.  
* A .filelist.txt file is utilized to diligently track which files have already been indexed, ensuring that the database is updated only when necessary.

### **3\. ğŸ” Query & Retrieval**

* When you pose a question, it is first embedded and then intelligently compared against the multitude of stored vectors.  
* The top-matching document chunks are precisely retrieved, forming the crucial **context** for your query.  
* This retrieved context is then seamlessly fed into your Large Language Model (LLM) prompt, typically formatted as:

Use the following context to answer:  
\[retrieved text\]

Question: \[your query\]

## ---

**ğŸ§ª Example**

Imagine your documents contain detailed information about photosynthesis. If you then ask:

Plaintext

What's photosynthesis?

The script will efficiently locate and return the most relevant chunks from your documents, even if your exact question wasn't explicitly present.

## ---

**ğŸ“‚ Folder Structure**

Bash

rag/  
â”œâ”€â”€ data/               \# ğŸ“ Your input .txt files  
â”‚   â””â”€â”€ data.txt  
â”œâ”€â”€ chroma\_db/          \# ğŸ“¦ Vector DB storage (auto-created)  
â”œâ”€â”€ .filelist.txt       \# ğŸ—‚ï¸ Tracks processed files (auto-generated)  
â”œâ”€â”€ rag.py              \# ğŸ§  Main RAG script  
â””â”€â”€ README.md           \# ğŸ“˜ This file

## **â–¶ï¸ Usage**

1. **Install dependencies**:  
   Bash  
   pip install sentence-transformers chromadb

2. **Add your .txt files** to the ./data folder.  
3. **Run**:  
   Bash  
   python3 rag.py

   * On the first run, the system will build and store the embeddings.  
   * On subsequent runs, it will intelligently skip reprocessing if the files haven't undergone any changes.

## ---

**ğŸ” Force Rebuilding the DB**

To trigger a complete reindex (e.g., after making edits to your files), simply delete .filelist.txt:

Bash

rm .filelist.txt

Alternatively, just add a new file to the ./data directory.

## ---

**ğŸ§  Future Enhancements**

* **Expanded File Support**: Integration for .pdf and .docx file formats.  
* **Superior Embedding Models**: Exploration and utilization of more advanced embedding models, such as nomic-embed-text-v1.5.  
* **Full LLM Integration**: Seamless integration with local LLMs (e.g., via llama.cpp).

---

Local context-aware question answering with semantic search ğŸ” \+ LLM ğŸ§ 

## ---

**ğŸ“š What is RAG?**

**RAG** \= **Retrieve** relevant chunks from documents â• **Augment** the LLM input.

This powerful combination significantly improves:

* ğŸ“Œ **Answer accuracy** based on your own data.  
* ğŸ” **Offline usability**.  
* âŒ **Reduced hallucination**.  
* âš¡ **Faster responses** for repeated queries.

## ---

**ğŸ› ï¸ How It Works (Simplified)**

### **1\. âœ‚ï¸ Chunk & Embed**

Documents are split into manageable chunks, and their embeddings are created using sentence-transformers.

### **2\. ğŸ§  Store in Vector DB**

These embeddings are persistently stored in ChromaDB.

### **3\. ğŸ” Query and Retrieve**

Your query is embedded, the database is searched, and the most similar chunks are returned as **context**.

## ---

**ğŸ§ª Example**

**Query:** *"What's photosynthesis?"*

ğŸ§  Output context: (This would be the text retrieved from your documents that helps answer the question.)